{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following phrases:\n",
    "\n",
    "\"Titanic is a great movie.\"\n",
    "\n",
    "\n",
    "\"Titanic is not a great movie.\"\n",
    "\n",
    "\n",
    "\"Titanic is a movie.\"\n",
    "\n",
    "\n",
    "The phrases correspond to short movie reviews, and each one of them conveys different sentiments. \n",
    "\n",
    "\n",
    "For example, the first phrase denotes positive sentiment about the film Titanic while the second one treats the movie as not so great (negative sentiment). \n",
    "\n",
    "\n",
    "Take a look at the third one more closely. \n",
    "\n",
    "\n",
    "There is no such word in that phrase which can tell you about anything regarding the sentiment conveyed by it. \n",
    "\n",
    "\n",
    "Hence, that is an example of neutral sentiment.\n",
    "\n",
    "\n",
    "Now, from a strict machine learning point of view, this task is nothing but a supervised learning task. \n",
    "\n",
    "\n",
    "You will supply a bunch of phrases (with the labels of their respective sentiments) to the machine learning model, and you will test the model on unlabeled phrases.\n",
    "\n",
    "\n",
    "Take a look at this review:\n",
    "\n",
    "<img src='../img/sen1.JPG'>\n",
    "\n",
    "\n",
    "The next step which seems natural is to create a representation similar to the following:\n",
    "\n",
    "\n",
    "<img src='../img/sen2.JPG'>\n",
    "\n",
    "\n",
    "The above representation is nothing but a Bag-of-words representation. \n",
    "\n",
    "\n",
    "This is probably the most fundamental concepts in NLP and is the first step of doing any text classification problem.\n",
    "\n",
    "\n",
    "A bag-of-words representation of a document does not only contain specific words but all the unique words in a document and their frequencies of occurrences. \n",
    "\n",
    "\n",
    "A bag is a mathematical set here, so by the definition of a set, the bag does not contain any duplicate words.\n",
    "\n",
    "\n",
    "The words that you found out in the bag-of-words will now construct the feature set of your document. \n",
    "\n",
    "\n",
    "So, consider you a collection of many movie reviews (documents), and you have created bag-of-words representations for each one of them and preserved their labels (i.e., sentiments - +ve or -ve in this case). \n",
    "\n",
    "\n",
    "Your training set should look like:\n",
    "\n",
    "<img src='../img/sen3.JPG'>\n",
    "\n",
    "\n",
    "This representation is also known as Corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classification for sentiment analysis\n",
    "\n",
    "\n",
    "Naive Bayes classification is nothing but applying Bayes rules for forming classification probabilities.\n",
    "\n",
    "Let's first build the notion of general terms in Naive Bayes classifier the context of sentiment classification. \n",
    "\n",
    "### Bayes rule:\n",
    "\n",
    "<img src='../img/sen4.JPG'>\n",
    "\n",
    "In this case, the class comprises two sentiments. \n",
    "* Positive \n",
    "* Negative\n",
    "\n",
    "Let's study each term of the above image in details in this context.\n",
    "\n",
    "The RHS term P(c|d) is read as the probability of class c given a document d. This term is also known as Posterior.\n",
    "P(d|c) should be similar.\n",
    "\n",
    "\n",
    "The term which is shown as Prior is your original belief i.e., original label of the document being positive or negative (in terms of sentiments).\n",
    "\n",
    "\n",
    "The term Likelihood is the probability of a document d given a class c.\n",
    "\n",
    "\n",
    "Now think of the term Posterior as your updated rule or updated belief obtained by multiplying Prior and Likelihood.\n",
    "\n",
    "\n",
    "But what is Normalization Constant P(d)? \n",
    "\n",
    "\n",
    "This term is divided with the result produced by the multiplication to ensure the outcome can be presented in a probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\VIPUL.GAUR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Feature Extractor\n",
    "\n",
    "To limit the number of features that the classifier needs to process, you start by constructing a list of the 2000 most frequent words in the overall corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extractor\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task\n",
    "\n",
    "\n",
    "Now that we've defined the feature extractor. \n",
    "\n",
    "\n",
    "Now, we can use it to train a Naive Bayes classifier to predict the sentiments of new movie reviews. \n",
    "\n",
    "\n",
    "To check your classifier's performance, you will compute its accuracy on the test set. \n",
    "\n",
    "\n",
    "NLTK provides show_most_informative_features() to see which features the classifier found to be most informative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79% accuracy without any hyperparameter tuning seems to be great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.3 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      7.0 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      7.0 : 1.0\n",
      "       contains(martian) = True              neg : pos    =      7.0 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most important features as interpreted by Naive Bayes\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
